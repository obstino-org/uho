{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CTranslate2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctranslate2\n",
    "from ctranslate2.models import Whisper\n",
    "import whisper\n",
    "import time\n",
    "\n",
    "tok = whisper.tokenizer.get_tokenizer(multilingual=True)\n",
    "m = Whisper(model_path=\"./ct2_base_artur_best\")\n",
    "\n",
    "audio_path = \"./mnt/d/Music/asr/test_laptop_mic-loud.wav\"    # 16kHz sample rate\n",
    "mel_from_file = whisper.audio.log_mel_spectrogram(audio_path).unsqueeze(dim=0).numpy()\n",
    "print(mel_from_file.shape)\n",
    "features = ctranslate2.StorageView.from_array(mel_from_file)\n",
    "\n",
    "# correct_prompt = [50258, 50305, 50359, 50363]  # == <|startoftranscript|><|sl|><|transcribe|><|notimestamps|>\n",
    "correct_prompt = [50258, 50305, 50359]\n",
    "\n",
    "t1 = time.time_ns()\n",
    "out = m.generate(features=features, prompts=[correct_prompt], beam_size=1, return_scores=True)\n",
    "t2 = time.time_ns()\n",
    "#print(out[0].sequences_ids[0])\n",
    "print(\"Runtime: {:.2f}s\".format((t2-t1)/1e9), tok.decode(out[0].sequences_ids[0]))\n",
    "#print(m.detect_language(features))\n",
    "print(out[0].scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ONNX models using KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import whisper.tokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy\n",
    "import whisper\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "sess_encoder = ort.InferenceSession(\"./kv_onnx_encoder.onnx\", sess_options)\n",
    "sess_decoder = ort.InferenceSession(\"./kv_onnx_decoder.onnx\", sess_options)\n",
    "\n",
    "# prepare input data\n",
    "audio_path = \"./test_speech.wav\"    # 16kHz sample rate\n",
    "mel_from_file = whisper.audio.log_mel_spectrogram(audio_path)\n",
    "input_data = whisper.audio.pad_or_trim(mel_from_file, whisper.audio.N_FRAMES)\n",
    "input_data = np.expand_dims(input_data, 0)\n",
    "\n",
    "orig_input_id_list = [50258, 50305, 50359]\n",
    "\n",
    "# run encoder inference\n",
    "xa = sess_encoder.run(\n",
    "    [\"audio\"],  # output name\n",
    "    { \"mel\": input_data }\n",
    ")\n",
    "\n",
    "# run first decoder inference\n",
    "shape_empty_cache_self_attn = shape_empty_cache_cross_attn = (12, 1, 0, 512)\n",
    "\n",
    "cache_self_attn = np.zeros(shape_empty_cache_self_attn, dtype=\"float32\")\n",
    "cache_cross_attn = np.zeros(shape_empty_cache_cross_attn, dtype=\"float32\")\n",
    "\n",
    "x_tokens = np.expand_dims(np.array(orig_input_id_list, dtype=np.int64), 0)\n",
    "x_audio = xa[0]\n",
    "\n",
    "t0 = time.time_ns()\n",
    "logits, cache_self_attn, cache_cross_attn = sess_decoder.run(\n",
    "    [\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "    {\n",
    "        \"tokens\": x_tokens,\n",
    "        \"audio\": x_audio,\n",
    "        \"cache_self_attn\": cache_self_attn,\n",
    "        \"cache_cross_attn\": cache_cross_attn,\n",
    "    },\n",
    ")\n",
    "t1 = time.time_ns()\n",
    "print(\"First inference time: {} ms\".format((t1-t0)/1e6))\n",
    "\n",
    "output_tokens = orig_input_id_list\n",
    "\n",
    "# run next decoder inferences\n",
    "last_token = logits[0, -1, :].argmax()\n",
    "input_token_tensor = np.expand_dims(np.array([last_token], dtype=np.int64), 0)\n",
    "output_tokens.append(last_token)\n",
    "\n",
    "#print(x_audio.shape)\n",
    "#print(x_audio[:, :0, :].shape)\n",
    "\n",
    "while last_token != 50257:\n",
    "    t0 = time.time_ns()\n",
    "    logits, cache_self_attn, cache_cross_attn = sess_decoder.run(\n",
    "        [\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        {\n",
    "            \"tokens\": input_token_tensor,\n",
    "            \"audio\": x_audio[:, :0, :],\n",
    "            \"cache_self_attn\": cache_self_attn,\n",
    "            \"cache_cross_attn\": cache_cross_attn\n",
    "        }\n",
    "    )\n",
    "    t1 = time.time_ns()\n",
    "    print(\"Subsequent inference time: {} ms\".format((t1-t0)/1e6))\n",
    "    print(\"logits: \", logits.shape)\n",
    "    print(\"self: \", cache_self_attn.shape)\n",
    "    print(\"cross: \", cache_cross_attn.shape)\n",
    "\n",
    "    last_token = logits[0, -1, :].argmax()\n",
    "    input_token_tensor = np.expand_dims(np.array([last_token], dtype=np.int64), 0)\n",
    "    output_tokens.append(last_token)\n",
    "\n",
    "tok = whisper.tokenizer.get_tokenizer(multilingual=True)\n",
    "print(tok.decode_with_timestamps(output_tokens))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
