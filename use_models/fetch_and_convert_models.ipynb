{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model = \"blko/whisper-base-sl-artur-full-ft\"\n",
    "revision = \"772cbcea0383a8f4359d3bd8457aa63ca881c47b\"\t# Training in progress, step 32000 (optimal model)\n",
    "token = None\n",
    "snapshot_download(repo_id=model, token=token, revision=revision, local_dir=\"./whisper-base-sl-artur-full-ft-best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to CTranslate2 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ctranslate2 OpenNMT-py==2.* sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ct2-transformers-converter --model ./whisper-base-sl-artur-full-ft-best --output_dir ./ct2_base_artur_best --copy_files tokenizer_config.json preprocessor_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to ONNX with KV caching  \n",
    "(massive help from https://cprohm.de/blog/whisper-full/ and https://cprohm.de/blog/whisper-full/convert.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper==20230117   # Important to use this older version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert huggingface model to openai pytorch format using \"convert_hf_to_openai.py\" (source: )\n",
    "!python convert_hf_to_openai.py --checkpoint ./whisper-base-sl-artur-full-ft-best --whisper_dump_path ./whisper-base-best-openai.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "def export():\n",
    "    model = whisper.load_model(\"./whisper-base-best-openai.pt\", device=\"cpu\")\n",
    "    model.eval()\n",
    "    patch(model)\n",
    "\n",
    "    encoder = model.encoder\n",
    "    decoder = FunctionalDecoder(model.decoder)\n",
    "\n",
    "    x_mel = torch.randn(1, 80, 3000)\n",
    "    x_tokens = torch.zeros((1, 10), dtype=torch.long)\n",
    "    x_audio = encoder(x_mel)\n",
    "\n",
    "    cache_self_attn = torch.zeros(\n",
    "        (len(decoder.keys_self_attn), 1, 0, model.dims.n_text_state),\n",
    "    )\n",
    "    cache_cross_attn = torch.zeros(\n",
    "        (len(decoder.keys_cross_attn), 1, 0, model.dims.n_audio_state),\n",
    "    )\n",
    "\n",
    "    print(\"self attn shape: \", cache_self_attn.shape)\n",
    "    print(\"cross attn shape: \", cache_cross_attn.shape)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (x_mel,),\n",
    "        \"./kv_onnx_encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 1: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=12\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (x_tokens, x_audio, cache_self_attn, cache_cross_attn),\n",
    "        \"./kv_onnx_decoder.onnx\",\n",
    "        input_names=[\"tokens\", \"audio\", \"cache_self_attn\", \"cache_cross_attn\"],\n",
    "        output_names=[\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        dynamic_axes={\n",
    "            # inputs\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {1: \"batch\", 2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {1: \"batch\", 2: \"cached_time\"},\n",
    "            # outputs\n",
    "            \"logits\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"new_cache_self_attn\": {1: \"batch\", 2: \"new_cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {1: \"batch\", 2: \"new_cached_time\"},\n",
    "        },\n",
    "        opset_version=12\n",
    "    )\n",
    "\n",
    "\n",
    "def patch(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx    # n_text_ctx = 448 in general\n",
    "\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx # n_audio_ctx = 1500\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, x, xa, cache_self_attn, cache_cross_attn):\n",
    "        # Create a dictionary that maps key() and value() functions to their respective cache values\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(x, xa, kv_cache=kv_cache)\n",
    "        return (\n",
    "            logits,\n",
    "            torch.stack([kv_cache[key] for key in self.keys_self_attn]),\n",
    "            torch.stack([kv_cache[key] for key in self.keys_cross_attn]),\n",
    "        )\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def _get_kv(self, x, xa=None, kv_cache=None):\n",
    "        xx = x if xa is None else xa\n",
    "        assert xx is not None\n",
    "\n",
    "        if kv_cache is None:\n",
    "            return self.key(xx), self.value(xx)\n",
    "\n",
    "        # append a key to kv_cache\n",
    "        key = torch.concat([kv_cache[self.key], self.key(xx).detach()], dim=1)\n",
    "        # truncate kv cache to (at most) self.n_ctx length (this may be 1500 for n_audio_ctx or <= 448 for n_text_ctx)\n",
    "        key = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.key] = key\n",
    "\n",
    "        value = torch.concat([kv_cache[self.value], self.value(xx).detach()], dim=1)\n",
    "        value = value[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "export()\n",
    "\n",
    "# At this point, we have kv_onnx_encoder.onnx and kv_onnx_decoder.onnx models for Whisper inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
