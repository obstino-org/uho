{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code shown below converts our pretrained Whisper torch model to ONNX format with KV caching, using 10s context window (instead of 30s) for faster inference on Android devices for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, create conda environment with python 3.10:\n",
    "#   conda create -n whisper2tflite python=3.10\n",
    "\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "# Source: https://colab.research.google.com/github/usefulsensors/openai-whisper/blob/main/notebooks/whisper_encoder_decoder_tflite.ipynb\n",
    "#!pip install onnx\n",
    "#!pip install onnxruntime\n",
    "#!pip install transformers\n",
    "#!pip install openai-whisper==20230117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model = \"blko/whisper-base-sl-artur-full-ft\"\n",
    "revision = \"772cbcea0383a8f4359d3bd8457aa63ca881c47b\"\t# Training in progress, step 32000 (optimal model)\n",
    "token = None\n",
    "snapshot_download(repo_id=model, token=token, revision=revision, local_dir=\"./whisper-base-sl-artur-full-ft-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    }
   ],
   "source": [
    "# REFERENCE CODE FOR CONTEXT REDUCTION: https://github.com/sanchit-gandhi/codesnippets/blob/main/whisper-reduce-context.ipynb\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperConfig, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "model_name = \"./whisper-base-sl-artur-full-ft-best\"\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "state_dict = model.state_dict()\n",
    "state_dict[\"model.encoder.embed_positions.weight\"] = state_dict[\"model.encoder.embed_positions.weight\"][:500, :]\n",
    "\n",
    "# now load these weights back into the Whisper model, this time configured for this new seq len\n",
    "config = WhisperConfig.from_pretrained(model_name, max_source_positions=500)\n",
    "model = WhisperForConditionalGeneration(config)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.save_pretrained(\"./whisper-base-sl-artur-full-ft-best-ctx10s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly test the model with 10s context window on sample recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ampak samo to še zdaleč ni dovolj, če se ozremo po domačem osnovnčju te pogoje konec koncev izpolnjujeta tudi Venera in Mars, pa zato ... ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperConfig, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name)\n",
    "# set the input length to 10 seconds\n",
    "feature_extractor = WhisperFeatureExtractor(chunk_length=10)\n",
    "# combine to form the processor\n",
    "processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# check model works on a given sample\n",
    "audio_path = \"./test.wav\"\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "if audio.shape[0] > 16000*10:\n",
    "    audio = audio[0:int(16000*9.5)]\n",
    "\n",
    "input_features = torch.asarray(processor.feature_extractor(audio)[\"input_features\"]).to(device)\n",
    "pred_ids = model.generate(input_features, max_new_tokens=128)\n",
    "pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF model path: ./whisper-base-sl-artur-full-ft-best-ctx10s\n",
      "OpenAI model path: ./whisper-base-best-openai-ctx10s.pt\n"
     ]
    }
   ],
   "source": [
    "!python convert_hf_to_openai.py --checkpoint ./whisper-base-sl-artur-full-ft-best-ctx10s --whisper_dump_path ./whisper-base-best-openai-ctx10s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model that allows KV-caching, export as Onnx\n",
    "(massive help from https://cprohm.de/blog/whisper-full/ and https://cprohm.de/blog/whisper-full/convert.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self attn shape:  torch.Size([12, 1, 0, 512])\n",
      "cross attn shape:  torch.Size([12, 1, 0, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bko/miniconda3/envs/pt/lib/python3.10/site-packages/whisper/model.py:153: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
      "/tmp/ipykernel_2200/4045988563.py:86: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
      "/tmp/ipykernel_2200/4045988563.py:87: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import whisper\n",
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "\n",
    "def export():\n",
    "    model = whisper.load_model(\"./whisper-base-best-openai-ctx10s.pt\", device=\"cpu\")\n",
    "    model.eval()\n",
    "    patch(model)\n",
    "\n",
    "    encoder = model.encoder\n",
    "    decoder = FunctionalDecoder(model.decoder)\n",
    "\n",
    "    x_mel = torch.randn(1, 80, 1000)\n",
    "    x_tokens = torch.zeros((1, 10), dtype=torch.long)\n",
    "    x_audio = encoder(x_mel)\n",
    "\n",
    "    cache_self_attn = torch.zeros(\n",
    "        (len(decoder.keys_self_attn), 1, 0, model.dims.n_text_state),\n",
    "    )\n",
    "    cache_cross_attn = torch.zeros(\n",
    "        (len(decoder.keys_cross_attn), 1, 0, model.dims.n_audio_state),\n",
    "    )\n",
    "\n",
    "    print(\"self attn shape: \", cache_self_attn.shape)\n",
    "    print(\"cross attn shape: \", cache_cross_attn.shape)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        (x_mel,),\n",
    "        \"./ctx10encoder.onnx\",\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"audio\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {0: \"batch\", 1: \"time\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "        },\n",
    "        opset_version=12\n",
    "    )\n",
    "\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (x_tokens, x_audio, cache_self_attn, cache_cross_attn),\n",
    "        \"./ctx10decoder.onnx\",\n",
    "        input_names=[\"tokens\", \"audio\", \"cache_self_attn\", \"cache_cross_attn\"],\n",
    "        output_names=[\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        dynamic_axes={\n",
    "            # inputs\n",
    "            \"tokens\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"audio\": {0: \"batch\", 1: \"time\"},\n",
    "            \"cache_self_attn\": {1: \"batch\", 2: \"cached_seq\"},\n",
    "            \"cache_cross_attn\": {1: \"batch\", 2: \"cached_time\"},\n",
    "            # outputs\n",
    "            \"logits\": {0: \"batch\", 1: \"seq\"},\n",
    "            \"new_cache_self_attn\": {1: \"batch\", 2: \"new_cached_seq\"},\n",
    "            \"new_cache_cross_attn\": {1: \"batch\", 2: \"new_cached_time\"},\n",
    "        },\n",
    "        opset_version=12\n",
    "    )\n",
    "\n",
    "\n",
    "def patch(model):\n",
    "    for block in model.decoder.blocks:\n",
    "        block.attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.attn.n_ctx = model.dims.n_text_ctx\n",
    "\n",
    "        block.cross_attn.__class__ = FunctionalMultiHeadAttention\n",
    "        block.cross_attn.n_ctx = model.dims.n_audio_ctx\n",
    "\n",
    "\n",
    "class FunctionalDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.keys_self_attn = []\n",
    "        self.keys_cross_attn = []\n",
    "\n",
    "        for block in decoder.blocks:\n",
    "            self.keys_self_attn += (block.attn.key, block.attn.value)\n",
    "            self.keys_cross_attn += (block.cross_attn.key, block.cross_attn.value)\n",
    "\n",
    "    def forward(self, x, xa, cache_self_attn, cache_cross_attn):\n",
    "        kv_cache = {\n",
    "            **dict(zip(self.keys_self_attn, cache_self_attn)),\n",
    "            **dict(zip(self.keys_cross_attn, cache_cross_attn)),\n",
    "        }\n",
    "\n",
    "        logits = self.decoder(x, xa, kv_cache=kv_cache)\n",
    "        return (\n",
    "            logits,\n",
    "            torch.stack([kv_cache[key] for key in self.keys_self_attn]),\n",
    "            torch.stack([kv_cache[key] for key in self.keys_cross_attn]),\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionalMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        k, v = self._get_kv(x, xa, kv_cache)\n",
    "\n",
    "        q = self.query(x)\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def _get_kv(self, x, xa=None, kv_cache=None):\n",
    "        xx = x if xa is None else xa\n",
    "        assert xx is not None\n",
    "\n",
    "        if kv_cache is None:\n",
    "            return self.key(xx), self.value(xx)\n",
    "\n",
    "        key = torch.concat([kv_cache[self.key], self.key(xx).detach()], dim=1)\n",
    "        key = key[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.key] = key\n",
    "\n",
    "        value = torch.concat([kv_cache[self.value], self.value(xx).detach()], dim=1)\n",
    "        value = value[:, -self.n_ctx :, :]\n",
    "        kv_cache[self.value] = value\n",
    "\n",
    "        return kv_cache[self.key], kv_cache[self.value]\n",
    "\n",
    "export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample onnx inference using base Whisper model with KV caching and 10s context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder inference time: 121.293919 ms\n",
      "First inference time: 130.769679 ms\n",
      "Subsequent inference time: 80.017412 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 4, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.488141 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 5, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.271758 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 6, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.634089 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 7, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 21.0186 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 8, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.632377 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 9, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.704873 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 10, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.053787 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 11, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.554508 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 12, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.474084 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 13, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 14.944029 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 14, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 14.413312 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 15, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.766009 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 16, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.997296 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 17, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.266752 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 18, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.910945 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 19, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 20.013053 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 20, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.806362 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 21, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.932986 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 22, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.650579 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 23, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.898211 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 24, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.102258 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 25, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.326845 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 26, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.903528 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 27, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.181799 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 28, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.221133 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 29, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.789162 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 30, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 27.146274 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 31, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 18.174101 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 32, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.477662 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 33, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 20.175722 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 34, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.036443 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 35, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.922955 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 36, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.748697 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 37, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.668775 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 38, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.338897 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 39, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.283663 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 40, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 40.063666 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 41, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 20.912189 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 42, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.843688 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 43, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.45548 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 44, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.386998 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 45, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.248716 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 46, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.040962 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 47, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.012095 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 48, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.43419 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 49, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.735859 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 50, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.603542 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 51, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.987159 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 52, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.348914 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 53, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 23.866338 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 54, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 17.576366 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 55, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 15.805734 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 56, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.800904 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 57, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.439338 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 58, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "Subsequent inference time: 16.244929 ms\n",
      "logits:  (1, 1, 51865)\n",
      "self:  (12, 1, 59, 512)\n",
      "cross:  (12, 1, 500, 512)\n",
      "<|startoftranscript|><|sl|><|transcribe|>Ampak samo to še zdaleč ni dovolj, če se ozremo po domačem osnovnčju te pogoje konec koncev izpolnjujeta tudi Venera in Mars. <|9.00|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import whisper.tokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy\n",
    "import whisper\n",
    "import numpy as np\n",
    "import time\n",
    "import librosa\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "sess_encoder = ort.InferenceSession(\"./ctx10encoder.onnx\", sess_options)\n",
    "sess_decoder = ort.InferenceSession(\"./ctx10decoder.onnx\", sess_options)\n",
    "\n",
    "# prepare input data\n",
    "audio_path = \"./test.wav\"\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "if audio.shape[0] > 16000*10:\n",
    "    audio = audio[0:int(16000*9)]   # trim to 9 seconds -- helps prevent some hallucinations\n",
    "\n",
    "audio = whisper.audio.pad_or_trim(audio, 16000*10)\n",
    "mel_from_file = whisper.audio.log_mel_spectrogram(audio)\n",
    "input_data = np.expand_dims(mel_from_file, 0)\n",
    "\n",
    "orig_input_id_list = [50258, 50305, 50359]\n",
    "\n",
    "# run encoder inference\n",
    "t0 = time.time_ns()\n",
    "xa = sess_encoder.run(\n",
    "    [\"audio\"],  # output name\n",
    "    { \"mel\": input_data }\n",
    ")\n",
    "t1 = time.time_ns()\n",
    "print(\"Encoder inference time: {} ms\".format((t1-t0)/1e6))\n",
    "\n",
    "# run first decoder inference\n",
    "shape_empty_cache_self_attn = shape_empty_cache_cross_attn = (12, 1, 0, 512)\n",
    "\n",
    "cache_self_attn = np.zeros(shape_empty_cache_self_attn, dtype=\"float32\")\n",
    "cache_cross_attn = np.zeros(shape_empty_cache_cross_attn, dtype=\"float32\")\n",
    "\n",
    "x_tokens = np.expand_dims(np.array(orig_input_id_list, dtype=np.int64), 0)\n",
    "x_audio = xa[0]\n",
    "\n",
    "t0 = time.time_ns()\n",
    "logits, cache_self_attn, cache_cross_attn = sess_decoder.run(\n",
    "    [\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "    {\n",
    "        \"tokens\": x_tokens,\n",
    "        \"audio\": x_audio,\n",
    "        \"cache_self_attn\": cache_self_attn,\n",
    "        \"cache_cross_attn\": cache_cross_attn,\n",
    "    },\n",
    ")\n",
    "t1 = time.time_ns()\n",
    "print(\"First inference time: {} ms\".format((t1-t0)/1e6))\n",
    "\n",
    "output_tokens = orig_input_id_list\n",
    "\n",
    "# run next decoder inferences\n",
    "last_token = logits[0, -1, :].argmax()\n",
    "input_token_tensor = np.expand_dims(np.array([last_token], dtype=np.int64), 0)\n",
    "output_tokens.append(last_token)\n",
    "\n",
    "#print(x_audio.shape)\n",
    "#print(x_audio[:, :0, :].shape)\n",
    "\n",
    "count = 0\n",
    "while last_token != 50257 and count < 75:\n",
    "    count += 1\n",
    "    t0 = time.time_ns()\n",
    "    logits, cache_self_attn, cache_cross_attn = sess_decoder.run(\n",
    "        [\"logits\", \"new_cache_self_attn\", \"new_cache_cross_attn\"],\n",
    "        {\n",
    "            \"tokens\": input_token_tensor,\n",
    "            \"audio\": x_audio[:, :0, :],\n",
    "            \"cache_self_attn\": cache_self_attn,\n",
    "            \"cache_cross_attn\": cache_cross_attn\n",
    "        }\n",
    "    )\n",
    "    t1 = time.time_ns()\n",
    "    print(\"Subsequent inference time: {} ms\".format((t1-t0)/1e6))\n",
    "    print(\"logits: \", logits.shape)\n",
    "    print(\"self: \", cache_self_attn.shape)\n",
    "    print(\"cross: \", cache_cross_attn.shape)\n",
    "\n",
    "    last_token = logits[0, -1, :].argmax()\n",
    "    input_token_tensor = np.expand_dims(np.array([last_token], dtype=np.int64), 0)\n",
    "    output_tokens.append(last_token)\n",
    "\n",
    "tok = whisper.tokenizer.get_tokenizer(multilingual=True)\n",
    "print(tok.decode_with_timestamps(output_tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
